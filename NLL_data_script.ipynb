{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLL data conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# OPTIONS\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# give the dataset an identifier\n",
    "dataset_name = \"D7\"\n",
    "# if high-residual picks are to be removed post-inversion, provide name of the model\n",
    "model_name = \"reyk_final_model\" \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# define model bounds\n",
    "lat_max = 64.05\n",
    "lat_min = 63.8\n",
    "lon_max = 338.15\n",
    "lon_min = 337.38\n",
    "depth_min = 1\n",
    "depth_max = 16\n",
    "# exclude all events outside model bounds (should always be True)\n",
    "event_bounds_filtering = True\n",
    "# exclude all stations outside model bounds (should always be True)\n",
    "station_bounds_filtering = True\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# pre-filter data based on event quality statistics calculated by NonLinLoc\n",
    "NLL_filtering = True\n",
    "# choose whether to filter based on NLL RMS value (if True then choose a reasonably high value to begin with since the original data will contain duplicate picks etc)\n",
    "NLL_RMS_filtering = True\n",
    "NLL_RMS_max = 0.15\n",
    "# choose whether to filter based on NLL maximum azimuthal gap (should always be set to True)\n",
    "NLL_gap_filtering = True\n",
    "NLL_max_gap = 180\n",
    "# choose whether to filter based on NLL minimum number of arrivals per event (should always be set to True)\n",
    "NLL_nphs_filtering = True\n",
    "NLL_picks_min = 5\n",
    "# choose whether to filter based on NLL minimum event-station distance (should always be set to True)\n",
    "NLL_dist_filtering = True\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# choose whether to filter events based on updated RMS value (noting that the recalculated values use residuals and weights with less decimal places from the event files - choose a reasonable value)\n",
    "recalc_RMS_filtering = False\n",
    "recalc_RMS_max = 0.15\n",
    "# choose whether to filter events based on updated maximum azimuthal gap (should always be set to True)\n",
    "recalc_gap_filtering = True\n",
    "recalc_max_gap = 180\n",
    "# choose whether to filter events based on updated minimum number of arrivals per event (should always be set to True)\n",
    "recalc_nphs_filtering = True\n",
    "recalc_picks_min = 5\n",
    "# choose whether to filter events based on minimum number of stations\n",
    "no_of_stations_filtering = True\n",
    "stations_min = 5\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# choose whether to set a minimum number of P-picks per event\n",
    "P_nphs_filtering = False\n",
    "P_picks_min = 5\n",
    "# choose whether to remove events which have a max azimuthal gap greater than 180 degrees between P-arrivals (used when testing P only relocation)\n",
    "calculate_max_gap_P = False\n",
    "\n",
    "# choose whether to set a minimum number of S-picks per event\n",
    "S_nphs_filtering = False\n",
    "S_picks_min = 5\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# if high-residual picks have been identified from a previous model run, choose whether to remove them from input data\n",
    "remove_high_residual_picks = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FILE AND DIRECTORY PATHS\n",
    "\n",
    "data_directory = \"/home/hra35/Documents/events\" # parent data directory\n",
    "\n",
    "path_to_stations = os.path.join(data_directory, \"station_coordinates.txt\") # path to the file containing station coordinates, from Tim\n",
    "\n",
    "path_to_tim_picks = os.path.join(data_directory, \"tim_picks\") # directory to Tim's pick files\n",
    "path_to_esme_picks = os.path.join(data_directory, \"esme_picks\") #  directory to Esme's pick files\n",
    "path_to_hr_picks = os.path.join(data_directory, \"hr_picks\") #  directory to my pick files\n",
    "path_to_all_picks = os.path.join(data_directory, \"all_picks\") # directory to all pick files combined\n",
    "\n",
    "path_to_nll_script_outputs = os.path.join(data_directory, \"nll_script_outputs\") # parent destination directory for any output files by the data conversion script\n",
    "\n",
    "path_to_model_nll_script_outputs = os.path.join(path_to_nll_script_outputs, \"%s\" %dataset_name) # model directory within the destination file output directory\n",
    "if not os.path.exists(path_to_model_nll_script_outputs):\n",
    "    os.makedirs(path_to_model_nll_script_outputs)\n",
    "\n",
    "path_to_fmtomo_files = os.path.join(path_to_model_nll_script_outputs, \"fmtomo_inputs\") # path to directory where all generated FMTOMO input files will be saved\n",
    "if not os.path.exists(path_to_fmtomo_files):\n",
    "    os.makedirs(path_to_fmtomo_files)\n",
    "    \n",
    "path_to_residuals = os.path.join(data_directory, \"residuals\") # path to parent directory containing all residual calculation files\n",
    "path_to_model_residuals = os.path.join(path_to_residuals, \"%s\" %model_name) # path to directory containing residual files for the relevant model\n",
    "if not os.path.exists(path_to_model_residuals):\n",
    "    os.makedirs(path_to_model_residuals)\n",
    "\n",
    "path_to_high_residual_picks_P = os.path.join(path_to_model_residuals, \"p_picks_rm.txt\") # text file with high-residual P-picks to remove\n",
    "path_to_high_residual_picks_S = os.path.join(path_to_model_residuals, \"s_picks_rm.txt\") # text file with high-residual S-picks to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of event files from Tim:  2003\n",
      "Number of event files from Esme:  196\n",
      "Number of event files from Hanna-Riia:  30\n",
      "Total number of event files with unique identifiers:  2215\n"
     ]
    }
   ],
   "source": [
    "# LIST ALL NLL PICK FILES\n",
    "\n",
    "# make a list of Tim's nonlinloc output files\n",
    "tim_nll_files = glob.glob(os.path.join(path_to_tim_picks, f'*{\".hyp\"}'))\n",
    "print(\"Number of event files from Tim: \", len(tim_nll_files))\n",
    "\n",
    "# make a list of Esme's nonlinloc output files\n",
    "esme_nll_files = glob.glob(os.path.join(path_to_esme_picks, f'*{\".hyp\"}'))\n",
    "print(\"Number of event files from Esme: \", len(esme_nll_files))\n",
    "\n",
    "# make a list of my nonlinloc output files\n",
    "hr_nll_files = glob.glob(os.path.join(path_to_hr_picks, f'*{\".hyp\"}'))\n",
    "print(\"Number of event files from Hanna-Riia: \", len(hr_nll_files))\n",
    "\n",
    "# copy all pick files to a shared folder, also removing obvious duplicates with the same name\n",
    "\n",
    "# function to copy files, checking for duplicates\n",
    "def copy(source_files, destination):\n",
    "    for source_file in source_files:\n",
    "        base_name = os.path.basename(source_file)\n",
    "        destination_file = os.path.join(destination, base_name)\n",
    "        if not os.path.exists(destination_file):\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "# copy Tim's files to the destination directory\n",
    "copy(tim_nll_files, path_to_all_picks)\n",
    "\n",
    "# copy Esme's files to the destination directory\n",
    "copy(esme_nll_files, path_to_all_picks)\n",
    "\n",
    "# copy my pick files to the destination directory\n",
    "copy(hr_nll_files, path_to_all_picks)\n",
    "\n",
    "# make a comprehensive list\n",
    "all_nll_files = glob.glob(os.path.join(path_to_all_picks, f'*{\".hyp\"}'))\n",
    "\n",
    "# check for empty files\n",
    "for file in all_nll_files:\n",
    "    if os.path.getsize(file) == 0:\n",
    "        # delete the file if it's empty\n",
    "        os.remove(file)\n",
    "        #print(f\"The file '{file}' was empty and has been deleted.\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# update the file list\n",
    "all_nll_files = glob.glob(os.path.join(path_to_all_picks, f'*{\".hyp\"}'))\n",
    "print(\"Total number of event files with unique identifiers: \", len(all_nll_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN EVENT DATA\n",
    "\n",
    "# make a copy of the full list (will be edited as events are removed)\n",
    "nll_files = all_nll_files\n",
    "\n",
    "# iterate through the files to get event id-s from filenames\n",
    "event_id_list = []\n",
    "for file in nll_files:\n",
    "    event_id = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_id_list.append(event_id)\n",
    "    \n",
    "# initialise the event dataframe\n",
    "event_df = pd.DataFrame(data=None, columns=[\"event_ID\",\n",
    "                                            \"lat\",\n",
    "                                            \"lon\",\n",
    "                                            \"depth\",\n",
    "                                            \"RMS\",\n",
    "                                            \"Gap\",\n",
    "                                            \"Dist\",\n",
    "                                            \"Nphs\",\n",
    "                                            \"Picker\",\n",
    "                                            \"No_stations\"])\n",
    "\n",
    "event_df[\"event_ID\"] = event_id_list\n",
    "    \n",
    "# read the event .hyp files and fill in the dataframe with coordinates and NLL statistics values\n",
    "for file in nll_files:\n",
    "    with open(file, 'r') as file_to_read:\n",
    "        lines = file_to_read.readlines()\n",
    "        for line in lines:\n",
    "            \n",
    "            if line.startswith(\"GEOGRAPHIC\"):\n",
    "                entries = line.split()\n",
    "                latitude = entries[9]\n",
    "                longitude = entries[11]\n",
    "                depth = entries[13]\n",
    "\n",
    "            if line.startswith(\"QUALITY\"):\n",
    "                entries = line.split()\n",
    "                rms = entries[8]\n",
    "                nphs = entries[10]\n",
    "                gap = entries[12]\n",
    "                dist = entries[14]\n",
    "            \n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_df.loc[event_df[\"event_ID\"] == file_name, [\"lat\", \"lon\", \"depth\", \"RMS\", \"Gap\", \"Dist\", \"Nphs\"]] = [latitude, longitude, depth, rms, gap, dist, nphs]\n",
    "    \n",
    "# convert dataframe entries to floats\n",
    "numerical_columns = ['lat', 'lon', 'depth', 'RMS', \"Gap\", \"Dist\", \"Nphs\", \"No_stations\"]\n",
    "for column in numerical_columns:\n",
    "    event_df[column] = event_df[column].astype(float)\n",
    "    event_df[column] = event_df[column].round(6)\n",
    "    \n",
    "# convert event longitudes to Eastward coordinates (for FMTOMO)\n",
    "event_df['lon'] = event_df['lon'] + 360\n",
    "\n",
    "# save the full unfiltered dataframe for later\n",
    "full_event_df = event_df.copy()\n",
    "\n",
    "#print(event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2173\n"
     ]
    }
   ],
   "source": [
    "# FILTER EVENTS BASED ON MODEL EXTENT\n",
    "\n",
    "if event_bounds_filtering == True:\n",
    "    \n",
    "    # filter dataframe, removing all events outside model region\n",
    "    filtered_df = event_df[(event_df['lon'] >= lon_min) & (event_df['lon'] <= lon_max) & (event_df['lat'] >= lat_min) & (event_df['lat'] <= lat_max) & (event_df['depth'] >= depth_min) & (event_df['depth'] <= depth_max)]\n",
    "    event_df = filtered_df\n",
    "\n",
    "    # update list of event files to read after applying filter    \n",
    "    updated_file_list = []\n",
    "    for file in nll_files:\n",
    "        event = os.path.splitext(os.path.basename(file))[0]\n",
    "        event_in_range = event in event_df['event_ID'].values\n",
    "        if event_in_range == True:\n",
    "            updated_file_list.append(file)\n",
    "        else:\n",
    "            pass\n",
    "    nll_files = updated_file_list\n",
    "\n",
    "print(len(nll_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events removed based on NLL max. azimuthal gap value:  377\n",
      "Number of events removed based on NLL number of picks:  7\n",
      "Number of events removed based on NLL minimum event-station distance:  123\n",
      "Number of events removed based on NLL RMS error:  17\n",
      "Total number of events left after preliminary NLL quality filtering:  1649\n"
     ]
    }
   ],
   "source": [
    "# FILTER EVENTS BASED ON NLL QUALITY STATISTICS\n",
    "\n",
    "if NLL_filtering == True:\n",
    "\n",
    "    # filter based on max azimuthal gap value from NLL\n",
    "    \n",
    "    if NLL_gap_filtering == True:\n",
    "        \n",
    "        # filter dataframe\n",
    "        init = len(event_df)\n",
    "        filtered_df = event_df[(event_df['Gap'] < NLL_max_gap)]\n",
    "        event_df = filtered_df\n",
    "        fin = len(event_df)\n",
    "        removed = init - fin\n",
    "\n",
    "        # update list of event files to read after applying filter    \n",
    "        updated_file_list = []\n",
    "        for file in nll_files:\n",
    "            event = os.path.splitext(os.path.basename(file))[0]\n",
    "            event_in_range = event in event_df['event_ID'].values\n",
    "            if event_in_range == True:\n",
    "                updated_file_list.append(file)\n",
    "        nll_files = updated_file_list\n",
    "        print(\"Number of events removed based on NLL max. azimuthal gap value: \", removed)\n",
    "        \n",
    "    # filter based on total number of picks per event from NLL\n",
    "\n",
    "    if NLL_nphs_filtering == True:\n",
    "    \n",
    "        # filter dataframe\n",
    "        init = len(event_df)\n",
    "        filtered_df = event_df[(event_df[\"Nphs\"] > NLL_picks_min)]\n",
    "        event_df = filtered_df\n",
    "        fin = len(event_df)\n",
    "        removed = init - fin\n",
    "\n",
    "        # update list of event files to read after applying filter    \n",
    "        updated_file_list = []\n",
    "        for file in nll_files:\n",
    "            event = os.path.splitext(os.path.basename(file))[0]\n",
    "            event_in_range = event in event_df['event_ID'].values\n",
    "            if event_in_range == True:\n",
    "                updated_file_list.append(file)\n",
    "        nll_files = updated_file_list\n",
    "        print(\"Number of events removed based on NLL number of picks: \", removed)\n",
    "       \n",
    "    # filter events based on minimum event-station distance \n",
    "\n",
    "    if NLL_dist_filtering == True:\n",
    "    \n",
    "        # filter dataframe, removing all events with minimum event-station distance greater than event depth\n",
    "        init = len(event_df)\n",
    "        dist_mask = (event_df[\"Dist\"] > event_df[\"depth\"])\n",
    "        event_df = event_df[~dist_mask]\n",
    "        fin = len(event_df)\n",
    "        removed = init - fin\n",
    "    \n",
    "        # update list of event files to read after applying filter    \n",
    "        updated_file_list = []\n",
    "        for file in nll_files:\n",
    "            event = os.path.splitext(os.path.basename(file))[0]\n",
    "            event_in_range = event in event_df['event_ID'].values\n",
    "            if event_in_range == True:\n",
    "                updated_file_list.append(file)\n",
    "        nll_files = updated_file_list\n",
    "        print(\"Number of events removed based on NLL minimum event-station distance: \", removed)\n",
    "        \n",
    "    # filter based on RMS error from NLL\n",
    "        \n",
    "    if NLL_RMS_filtering == True:\n",
    "    \n",
    "        # filter dataframe\n",
    "        init = len(event_df)\n",
    "        filtered_df = event_df[(event_df['RMS'] <= NLL_RMS_max)]\n",
    "        event_df = filtered_df\n",
    "        fin = len(event_df)\n",
    "        removed = init - fin\n",
    "\n",
    "        # update list of event files to read after applying filter    \n",
    "        updated_file_list = []\n",
    "        for file in nll_files:\n",
    "            event = os.path.splitext(os.path.basename(file))[0]\n",
    "            event_in_range = event in event_df['event_ID'].values\n",
    "            if event_in_range == True:\n",
    "                updated_file_list.append(file)\n",
    "        nll_files = updated_file_list\n",
    "\n",
    "        print(\"Number of events removed based on NLL RMS error: \", removed)\n",
    "    \n",
    "    # write an updated event file after filtering based on NLL for GMT\n",
    "    event_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"nll_filtered_events.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "    \n",
    "    print(\"Total number of events left after preliminary NLL quality filtering: \", len(event_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stations in the network:  54\n",
      "Number of stations in model region:  48\n"
     ]
    }
   ],
   "source": [
    "# READ IN STATIONS\n",
    "\n",
    "# read the station file into a dataframe\n",
    "station_df = pd.read_csv(path_to_stations, sep=r'\\s+', names = ['stat_ID', 'rm1', 'lat', 'lon', 'elev_km', 'rm2'])\n",
    "station_df.drop(['rm1', 'rm2'], axis=1, inplace=True)\n",
    "\n",
    "# remove stations that occur twice \n",
    "duplicate_mask = station_df.duplicated(subset='stat_ID', keep='last')\n",
    "station_df = station_df[~duplicate_mask]\n",
    "\n",
    "# convert station longitudes to Eastward coordinates\n",
    "station_df['lon'] = station_df['lon'] + 360\n",
    "\n",
    "# make a list of all station IDs\n",
    "station_id_list = station_df['stat_ID'].tolist()\n",
    "\n",
    "print(\"Total number of stations in the network: \", len(station_df))\n",
    "\n",
    "# remove stations not in model region\n",
    "if station_bounds_filtering == True:\n",
    "    \n",
    "    # filter dataframe, removing all stations outside model region\n",
    "    filtered_df = station_df[(station_df['lon'] >= lon_min) & (station_df['lon'] <= lon_max) & (station_df['lat'] >= lat_min) & (station_df['lat'] <= lat_max)]\n",
    "    station_df = filtered_df\n",
    "   \n",
    "    # update station list  \n",
    "    model_station_id_list = station_df['stat_ID'].tolist()\n",
    "    \n",
    "print(\"Number of stations in model region: \", len(model_station_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of event pick dataframes in dictionary is  1649\n"
     ]
    }
   ],
   "source": [
    "# READ IN PICK DATA FOR EACH EVENT\n",
    "\n",
    "# define the relevant keywords in the .hyp file\n",
    "start_word = \"PHASE\"\n",
    "end_word = \"END_PHASE\"\n",
    "\n",
    "# initialize a flag to indicate when to start and stop recording lines\n",
    "recording = False\n",
    "\n",
    "# initialise dictionary to store extracted lines per event file\n",
    "sta_data_dict = {}\n",
    "dict_keys = []\n",
    "\n",
    "for file in nll_files:\n",
    "    \n",
    "    Esme = False\n",
    "    Tim = False\n",
    "    HR = False\n",
    "    \n",
    "    #esme = \"Esme\"\n",
    "    #tim = \"Tim\"\n",
    "    #hr = \"Hanna-Riia\"\n",
    "    \n",
    "    # check whose pick file is being read\n",
    "    with open(file, 'r') as file_to_read:\n",
    "        content = file_to_read.read()\n",
    "        if \"Esme\" in content:\n",
    "            Esme = True\n",
    "            name = \"Esme\"\n",
    "        elif \"Tim\" in content:\n",
    "            Tim = True\n",
    "            name = \"Tim\"\n",
    "        else:\n",
    "            HR = True\n",
    "            name = \"Hanna-Riia\"\n",
    "                \n",
    "    # list to store the extracted lines\n",
    "    extracted_lines = []\n",
    "    with open(file, 'r') as file_to_read:\n",
    "        \n",
    "        for line in file_to_read:\n",
    "            # check if the line starts with the start_word\n",
    "            if line.strip().startswith(start_word):\n",
    "                recording = True\n",
    "                continue  # skip the line with the start_word itself\n",
    "            if recording == True:\n",
    "                # check if the line starts with the end_word to stop recording\n",
    "                if line.strip().startswith(end_word):\n",
    "                    recording = False\n",
    "                    continue  # skip the line with the end_word itself            \n",
    "                # add the line to the list of extracted lines\n",
    "                extracted_lines.append(line.strip())\n",
    "                \n",
    "                extracted_line_data = []\n",
    "                for data_line in extracted_lines:\n",
    "                    # split the line using the regex pattern (deals with any number of whitespaces between data values)\n",
    "                    line_data = re.split(r'\\s+', data_line)\n",
    "                    extracted_line_data.append(line_data)                    \n",
    "                \n",
    "                # save the extracted line data into a dataframe per event, format depending on whose pick file it is\n",
    "                if Tim == True or HR == True:\n",
    "                    df = pd.DataFrame(extracted_line_data, columns=[\"stat_ID\",\"Ins\",\"Cmp\",\"On\",\"Phase\",\"FM\",\"Date\",\"HrMn\",\"Sec\",\"Err\",\"ErrMag\",\"Coda\",\"Amp\",\"Per\",\"PriorWt\",\">\",\"TTpred\",\"Res\",\"Weight\",\"StaLocX\",\"StaLocY\",\"StaLocZ\",\"SDist\",\"SAzim\",\"RAz\",\"RDip\",\"RQual\",\"Tcorr\",\"TTerr\"])\n",
    "                    df.loc[:, 'event_ID'] = os.path.splitext(os.path.basename(file))[0]\n",
    "                    df = df[[\"event_ID\", \"stat_ID\",\"Ins\",\"Cmp\",\"On\",\"Phase\",\"FM\",\"Date\",\"HrMn\",\"Sec\",\"Err\",\"ErrMag\",\"Coda\",\"Amp\",\"Per\",\"PriorWt\",\">\",\"TTpred\",\"Res\",\"Weight\",\"StaLocX\",\"StaLocY\",\"StaLocZ\",\"SDist\",\"SAzim\",\"RAz\",\"RDip\",\"RQual\",\"Tcorr\",\"TTerr\"]]\n",
    "                if Esme == True:\n",
    "                    df = pd.DataFrame(extracted_line_data, columns=[\"stat_ID\",\"Ins\",\"Cmp\",\"On\",\"Phase\",\"FM\",\"Date\",\"HrMn\",\"Sec\",\"Err\",\"ErrMag\",\"Coda\",\"Amp\",\"Per\",\">\",\"TTpred\",\"Res\",\"Weight\",\"StaLocX\",\"StaLocY\",\"StaLocZ\",\"SDist\",\"SAzim\",\"RAz\",\"RDip\",\"RQual\",\"Tcorr\"])\n",
    "                    df.loc[:, 'event_ID'] = os.path.splitext(os.path.basename(file))[0]\n",
    "                    df = df[[\"event_ID\", \"stat_ID\",\"Ins\",\"Cmp\",\"On\",\"Phase\",\"FM\",\"Date\",\"HrMn\",\"Sec\",\"Err\",\"ErrMag\",\"Coda\",\"Amp\",\"Per\",\">\",\"TTpred\",\"Res\",\"Weight\",\"StaLocX\",\"StaLocY\",\"StaLocZ\",\"SDist\",\"SAzim\",\"RAz\",\"RDip\",\"RQual\",\"Tcorr\"]]\n",
    "                       \n",
    "    # save the dataframe into a dictionary with an event ID based key            \n",
    "    sta_data_dict[\"StaData_\" + os.path.splitext(os.path.basename(file))[0]] = df\n",
    "    \n",
    "    # make a list of the keys\n",
    "    key = (\"StaData_\" + os.path.splitext(os.path.basename(file))[0])\n",
    "    dict_keys.append(key)\n",
    "    \n",
    "    # save the name of the picker\n",
    "    condition = event_df[\"event_ID\"] == os.path.splitext(os.path.basename(file))[0]\n",
    "    event_df.loc[condition, \"Picker\"] = name\n",
    "\n",
    "print(\"Number of event pick dataframes in dictionary is \", len(sta_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of event pairs with origin time difference of less than 5 s: 323\n",
      "Number of identified duplicate event pairs:  323\n",
      "Total number of events after removing duplicates:  1326\n"
     ]
    }
   ],
   "source": [
    "# AT THIS STAGE, CHECK FOR (AND GET RID OF) DOUBLY PICKED EVENTS\n",
    "\n",
    "# get a list of event IDs as integers\n",
    "events = event_df[\"event_ID\"]\n",
    "events_int = [int(x) for x in events]\n",
    "events = events_int\n",
    "\n",
    "# identify events with similar origin times (difference of 5s)\n",
    "likely_duplicate_pairs = []\n",
    "for i in range(len(events)):\n",
    "    for j in range(i+1, len(events)):\n",
    "        if abs(events[i] - events[j]) < 5000:\n",
    "            likely_duplicate_pairs.append((events[i], events[j]))\n",
    "\n",
    "print(\"Number of event pairs with origin time difference of less than 5 s:\", len(likely_duplicate_pairs))\n",
    "#print(likely_duplicate_pairs)\n",
    "\n",
    "# compare the station dataframes of likely duplicates\n",
    "\n",
    "identical_pick_data = []\n",
    "different_pick_data = []\n",
    "\n",
    "for pair in likely_duplicate_pairs:\n",
    "    \n",
    "    # get event pick dataframes\n",
    "    event1 = pair[0]\n",
    "    event2 = pair[1]\n",
    "    key1 = (\"StaData_\" + str(event1))\n",
    "    key2 = (\"StaData_\" + str(event2))\n",
    "    sta_data_1 = sta_data_dict[key1]\n",
    "    sta_data_2 = sta_data_dict[key2]\n",
    "    \n",
    "    # sort in alphabetical order\n",
    "    sta_data_1 = sta_data_1.sort_values(by='stat_ID',ascending=True)\n",
    "    sta_data_1 = sta_data_1.reset_index(drop=True)\n",
    "    sta_data_2 = sta_data_2.sort_values(by='stat_ID',ascending=True)\n",
    "    sta_data_2 = sta_data_2.reset_index(drop=True)\n",
    "\n",
    "    # compare pick dataframes\n",
    "    if len(sta_data_1) == len(sta_data_2):\n",
    "        \n",
    "        # check if both dataframes have the same stations\n",
    "        comparison_result = sta_data_1[\"stat_ID\"].equals(sta_data_2[\"stat_ID\"])\n",
    "        \n",
    "        if comparison_result:\n",
    "            identical_pick_data.append(pair)\n",
    "        else:\n",
    "            different_pick_data.append(pair)\n",
    "            \n",
    "    else:\n",
    "        different_pick_data.append(pair)\n",
    "            \n",
    "print(\"Number of identified duplicate event pairs: \", len(identical_pick_data))\n",
    "#print(identical_pick_data)\n",
    "#print(\"Number of events to check further: \", len(different_pick_data)) # checked manually, not duplicates\n",
    "\n",
    "# get rid of one event per duplicate pair, keeping the one with lower NLL RMS uncertainty\n",
    "duplicates_to_remove = []\n",
    "for pair in identical_pick_data:\n",
    "    \n",
    "    event1 = str(pair[0])\n",
    "    event2 = str(pair[1])\n",
    "    \n",
    "    RMS_1 = event_df.loc[(event_df[\"event_ID\"] == event1), \"RMS\"].values[0]\n",
    "    RMS_2 = event_df.loc[(event_df[\"event_ID\"] == event2), \"RMS\"].values[0]\n",
    "    \n",
    "    if RMS_1 > RMS_2:\n",
    "        duplicates_to_remove.append(event1)\n",
    "        \n",
    "    else:\n",
    "        duplicates_to_remove.append(event2)\n",
    "\n",
    "# filter dataframe\n",
    "init = len(event_df)\n",
    "mask = (event_df[\"event_ID\"].isin(duplicates_to_remove))\n",
    "event_df = event_df[~mask]\n",
    "fin = len(event_df)\n",
    "removed = init - fin\n",
    "\n",
    "# update list of event files and list of dictionary keys after removing duplicates   \n",
    "updated_file_list = []\n",
    "keys = []\n",
    "for file in nll_files:\n",
    "    event = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_in_range = event in event_df['event_ID'].values\n",
    "    if event_in_range == True:\n",
    "        updated_file_list.append(file)\n",
    "        keys.append((\"StaData_\" + str(event)))\n",
    "nll_files = updated_file_list\n",
    "dict_keys = keys\n",
    "\n",
    "print(\"Total number of events after removing duplicates: \", len(event_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bad picks removed is  513\n"
     ]
    }
   ],
   "source": [
    "# EXTRA CLEANING STEPS TO REMOVE BAD PICKS AND ANY STATIONS OUTSIDE MODEL REGION\n",
    "\n",
    "# faulty picks\n",
    "inaccurate_picks = []\n",
    "zero_tt = []\n",
    "double_picked = []\n",
    "timing_error = []\n",
    "\n",
    "# loop through each event pick dataframe\n",
    "for key in dict_keys:\n",
    "\n",
    "    sta_data_original = sta_data_dict[key]\n",
    "    sta_data = sta_data_original.copy()\n",
    "    \n",
    "    # if stations have been excluded from the model region, remove picks from these stations\n",
    "    if (station_bounds_filtering == True) & (model_station_id_list != station_id_list):\n",
    "        mask = (sta_data[\"stat_ID\"].isin(model_station_id_list))\n",
    "        sta_data = sta_data[mask]\n",
    "        sta_data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "\n",
    "    # drop picks with a TT error of 10 000 (very tentatve picks)\n",
    "    sta_data['ErrMag'] = sta_data['ErrMag'].astype(float)\n",
    "    a = len(sta_data)\n",
    "    sta_data = sta_data[sta_data['ErrMag'] <= 1]\n",
    "    sta_data.reset_index(drop=True, inplace=True)\n",
    "    b = len(sta_data)\n",
    "    c = a-b\n",
    "    if c != 0:\n",
    "        inaccurate_picks.append(c)\n",
    "        \n",
    "    # drop picks with NLL residual >1s (will remove any outlier picks that still have timing errors, e.g. HELL, TKEL in 2021)\n",
    "    sta_data['Res'] = sta_data['Res'].astype(float)\n",
    "    a = len(sta_data)\n",
    "    sta_data = sta_data[abs(sta_data['Res']) < 1]\n",
    "    sta_data.reset_index(drop=True, inplace=True)\n",
    "    b = len(sta_data)\n",
    "    c = a-b\n",
    "    if c != 0:\n",
    "        timing_error.append(c)\n",
    "    \n",
    "    # drop picks with a TT of zero\n",
    "    sta_data['TTpred'] = sta_data['TTpred'].astype(float)\n",
    "    a = len(sta_data)\n",
    "    sta_data.loc[sta_data['TTpred'] == 0, 'TTpred'] = np.nan\n",
    "    sta_data = sta_data.dropna(subset=['TTpred'])\n",
    "    b = len(sta_data)\n",
    "    c = a-b\n",
    "    if c != 0:\n",
    "        zero_tt.append(c)\n",
    "                \n",
    "    # drop duplicate picks with same station and phase, keeping the pick with higher weight/lower residual\n",
    "    sta_data = sta_data.sort_values(by=['Weight'], ascending=[True])\n",
    "    a = len(sta_data)\n",
    "    sta_data.drop_duplicates(subset=['stat_ID', 'Phase'], keep='last', inplace=True)\n",
    "    sta_data.reset_index(drop=True, inplace=True)\n",
    "    b = len(sta_data)\n",
    "    c = a-b\n",
    "    if c != 0:\n",
    "        double_picked.append(c)       \n",
    "        \n",
    "    # save the new dataframe into dictionary\n",
    "    if sta_data.empty == False:\n",
    "        sta_data_dict[key] = sta_data\n",
    "    else:\n",
    "        print(\"Empty pick dataframe\")\n",
    "            \n",
    "print(\"Total number of bad picks removed is \", (sum(inaccurate_picks) + sum(timing_error) + sum(zero_tt) + sum(double_picked)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events:  1326\n",
      "Total number of picks:  27741\n"
     ]
    }
   ],
   "source": [
    "# MAKE A MASTER DATAFRAME OF ALL THE REMAINING PICKS\n",
    "\n",
    "# create an empty list to hold all event pick dataframes\n",
    "dfs = []\n",
    "\n",
    "# loop through the dictionary keys\n",
    "for key in dict_keys:\n",
    "    sta_data = sta_data_dict[key]\n",
    "    rows_to_append = sta_data[[\"event_ID\", \"stat_ID\", \"Phase\", \"TTpred\", \"ErrMag\", \"Res\", \"Weight\"]]\n",
    "    dfs.append(rows_to_append)\n",
    "\n",
    "# concatenate all event dataframes in the list\n",
    "all_picks_df = pd.concat(dfs, ignore_index=True)\n",
    "all_picks_df.rename(columns={'TTpred': 'TT_obs', 'ErrMag': 'TT_err', 'Res': 'NLL_res'}, inplace=True)\n",
    "\n",
    "# add event coordinates to the pick dataframe\n",
    "all_picks_df = all_picks_df.merge(event_df[['event_ID', 'lat', 'lon', 'depth', 'Nphs']], on='event_ID', how='left')\n",
    "\n",
    "# check for duplicate rows (shouldn't be an issue at this stage)\n",
    "all_picks_df.drop_duplicates(inplace=True)\n",
    "\n",
    "grouped_df = all_picks_df.groupby('event_ID')\n",
    "print(\"Total number of events: \", len(grouped_df))\n",
    "print(\"Total number of picks: \", len(all_picks_df))\n",
    "\n",
    "# save into file\n",
    "all_picks_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"picks.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "\n",
    "# make an unfiltered copy\n",
    "all_picks_df_full = all_picks_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF HIGH-RESIDUAL PICKS HAVE PREVIOUSLY BEEN IDENTIFIED, REMOVE THEM FROM INPUT DATA AT THIS POINT\n",
    "\n",
    "if remove_high_residual_picks == True:\n",
    "    \n",
    "    grouped_df = all_picks_df.groupby('event_ID')\n",
    "    print(\"Total number of events before removing high-residual picks: \", len(grouped_df))\n",
    "    print(\"Total number of picks before removing high-residual picks: \", len(all_picks_df))\n",
    "    \n",
    "    # read in P picks to remove\n",
    "    if os.path.exists(path_to_high_residual_picks_P):\n",
    "        p_to_rm = pd.read_csv(path_to_high_residual_picks_P, delimiter='\\t')\n",
    "        p_to_rm = p_to_rm[['event_ID', 'station_ID']]\n",
    "        print(\"Number of P-picks to remove:\", len(p_to_rm))\n",
    "        \n",
    "        # remove high residual P-picks from main dataframe\n",
    "        for index, row in p_to_rm.iterrows():\n",
    "        \n",
    "            event = str(row['event_ID'])\n",
    "            station = str(row['station_ID'])\n",
    "\n",
    "            p_mask = ((all_picks_df[\"event_ID\"] == event) & (all_picks_df[\"stat_ID\"] == station) & (all_picks_df[\"Phase\"] == \"P\"))    \n",
    "        \n",
    "            all_picks_df = all_picks_df[~p_mask]\n",
    "            all_picks_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # read in S picks to remove\n",
    "    if os.path.exists(path_to_high_residual_picks_S):\n",
    "        s_to_rm = pd.read_csv(path_to_high_residual_picks_S, delimiter='\\t')\n",
    "        s_to_rm = s_to_rm[['event_ID', 'station_ID']]\n",
    "        print(\"Number of S-picks to remove:\", len(s_to_rm))\n",
    "     \n",
    "        # remove high residual S-picks from main dataframe\n",
    "        for index, row in s_to_rm.iterrows():\n",
    "\n",
    "            event = str(row['event_ID'])\n",
    "            station_ID = str(row['station_ID'])\n",
    "        \n",
    "            s_mask = ((all_picks_df[\"event_ID\"] == event) & (all_picks_df[\"stat_ID\"] == station_ID) & (all_picks_df[\"Phase\"] == \"S\"))    \n",
    "\n",
    "            all_picks_df = all_picks_df[~s_mask]\n",
    "            all_picks_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \n",
    "    grouped_df = all_picks_df.groupby('event_ID')\n",
    "    print(\"Total number of events after removing high-residual picks: \", len(grouped_df))\n",
    "    print(\"Total number of picks after removing high-residual P-picks and S-picks: \", len(all_picks_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of events before removing events with too few arrivals:  1326\n",
      "Total no of picks left after removing events with few arrivals:  27741\n",
      "Total no of events left after removing events with few arrivals:  1326\n",
      "Total no of picks left after removing events with few arrivals:  27741\n"
     ]
    }
   ],
   "source": [
    "# CHECK HOW MANY ARRIVALS IN TOTAL ARE LEFT PER EVENT AND FILTER\n",
    "\n",
    "if recalc_nphs_filtering == True:\n",
    "\n",
    "    df = all_picks_df\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "    \n",
    "    print(\"Total no of events before removing events with too few arrivals: \", len(grouped_df))  \n",
    "    print(\"Total no of picks left after removing events with few arrivals: \", len(df))\n",
    "    \n",
    "    # check number of picks\n",
    "    events_to_keep = []\n",
    "    for group_name, group_data in grouped_df:\n",
    "        pick_count = len(group_data)\n",
    "        df.loc[df[\"event_ID\"] == group_name, \"Nphs\"] = pick_count\n",
    "        # update the number of arrivals in the event df\n",
    "        event_df.loc[event_df[\"event_ID\"] == group_name, \"Nphs\"] = pick_count\n",
    "        if pick_count >= recalc_picks_min:\n",
    "            events_to_keep.append(group_name)\n",
    "\n",
    "    # filter out events with fewer than the set number of picks\n",
    "    filtered_df = df[df['event_ID'].isin(events_to_keep)]\n",
    "    \n",
    "    all_picks_df = filtered_df\n",
    "    upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "\n",
    "print(\"Total no of events left after removing events with few arrivals: \", len(upd_grouped_df))  \n",
    "print(\"Total no of picks left after removing events with few arrivals: \", len(all_picks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of events before removing events with too few stations:  1326\n",
      "Total no of picks left after removing events with few stations:  27741\n",
      "Total no of events left after removing events with few stations:  1306\n",
      "Total no of picks left after removing events with few stations:  27603\n"
     ]
    }
   ],
   "source": [
    "# CHECK HOW MANY STATIONS ARE LEFT PER EVENT\n",
    "\n",
    "if no_of_stations_filtering == True:\n",
    "    \n",
    "    df = all_picks_df\n",
    "    \n",
    "    # group by event_ID\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "    print(\"Total no of events before removing events with too few stations: \", len(grouped_df))  \n",
    "    print(\"Total no of picks left after removing events with few stations: \", len(df))\n",
    "\n",
    "    # count the number of stations\n",
    "    to_keep = []\n",
    "    for group_name, group_data in grouped_df:\n",
    "        station_count = group_data['stat_ID'].nunique()\n",
    "        \n",
    "        # add the number of stations to the event df\n",
    "        isevent = event_df[\"event_ID\"] == group_name\n",
    "        event_df.loc[isevent, \"No_stations\"] = station_count\n",
    "        \n",
    "        if station_count >= stations_min:\n",
    "            to_keep.append(str(group_name))\n",
    "    \n",
    "    filtered_df = df[df['event_ID'].isin(to_keep)]\n",
    "    \n",
    "    all_picks_df = filtered_df\n",
    "    upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "    \n",
    "print(\"Total no of events left after removing events with few stations: \", len(upd_grouped_df))  \n",
    "print(\"Total no of picks left after removing events with few stations: \", len(all_picks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK HOW MANY P-ARRIVALS THERE ARE PER EVENT AND FILTER\n",
    "\n",
    "if P_nphs_filtering == True:\n",
    "    \n",
    "    print(\"Total no of events before removing events with too few P-arrivals: \", len(all_picks_df.groupby(\"event_ID\")))  \n",
    "    print(\"Total no of picks left after removing events with few P-arrivals: \", len(all_picks_df))\n",
    "    \n",
    "    df = all_picks_df[all_picks_df[\"Phase\"] == \"P\"]\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "   \n",
    "    # check number of P-picks\n",
    "    picks_count = grouped_df.size()\n",
    "\n",
    "    # filter out events with fewer than the set number of picks\n",
    "    events_to_keep = picks_count[picks_count >= P_picks_min]\n",
    "    filtered_df = all_picks_df[all_picks_df['event_ID'].isin(events_to_keep.index)]\n",
    "    \n",
    "    all_picks_df = filtered_df\n",
    "    upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "    \n",
    "    print(\"Total no of events left after removing events with few P-arrivals: \", len(upd_grouped_df))  \n",
    "    print(\"Total no of picks left after removing events with few P-arrivals: \", len(all_picks_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK HOW MANY S-ARRIVALS THERE ARE PER EVENT AND FILTER\n",
    "\n",
    "if S_nphs_filtering == True:\n",
    "    \n",
    "    print(\"Total no of events before removing events with too few S-arrivals: \", len(all_picks_df.groupby(\"event_ID\")))  \n",
    "    print(\"Total no of picks left after removing events with few S-arrivals: \", len(all_picks_df))\n",
    "    \n",
    "    df = all_picks_df[all_picks_df[\"Phase\"] == \"S\"]\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "   \n",
    "    # check number of P-picks\n",
    "    picks_count = grouped_df.size()\n",
    "\n",
    "    # filter out events with fewer than the set number of picks\n",
    "    events_to_keep = picks_count[picks_count >= S_picks_min]\n",
    "    filtered_df = all_picks_df[all_picks_df['event_ID'].isin(events_to_keep.index)]\n",
    "    \n",
    "    all_picks_df = filtered_df\n",
    "    upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "    \n",
    "    print(\"Total no of events left after removing events with few S-arrivals: \", len(upd_grouped_df))  \n",
    "    print(\"Total no of picks left after removing events with few S-arrivals: \", len(all_picks_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of events before removing events with high azimuthal gap:  1306\n",
      "Total no of picks left after removing events with high azimuthal gap:  27603\n",
      "Total no of events left after removing high azimuthal gap events:  1302\n",
      "Total no of picks left after removing high azimuthal gap events:  27518\n"
     ]
    }
   ],
   "source": [
    "# CHECK HOW MANY EVENTS LEFT HAVE MAX AZIMUTHAL GAP BELOW 180 AND FILTER\n",
    "\n",
    "if recalc_gap_filtering == True:\n",
    "   \n",
    "    # get some geopy functions to calulate azimuths \n",
    "    def calculate_azimuth(station_coords, epicenter_coords):\n",
    "        lat1, lon1 = math.radians(station_coords[0]), math.radians(station_coords[1])\n",
    "        lat2, lon2 = math.radians(epicenter_coords[0]), math.radians(epicenter_coords[1])\n",
    "        delta_lon = lon2 - lon1\n",
    "        x = math.sin(delta_lon) * math.cos(lat2)\n",
    "        y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(delta_lon))\n",
    "        azimuth_rad = math.atan2(x, y)\n",
    "        azimuth_deg = math.degrees(azimuth_rad)\n",
    "        return (azimuth_deg + 360) % 360\n",
    "    def calculate_azimuthal_gap(azimuths):\n",
    "        azimuths.sort()\n",
    "        gaps = [azimuths[i + 1] - azimuths[i] for i in range(len(azimuths) - 1)]\n",
    "        gaps.append(360 + azimuths[0] - azimuths[-1])\n",
    "        return max(gaps)\n",
    "    \n",
    "    df = all_picks_df\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "    \n",
    "    print(\"Total no of events before removing events with high azimuthal gap: \", len(grouped_df))  \n",
    "    print(\"Total no of picks left after removing events with high azimuthal gap: \", len(df))\n",
    "    \n",
    "    # function to calculate max gap for each group (event)\n",
    "    def calculate_max_gap(group):\n",
    "                \n",
    "        azimuths = []\n",
    "        event = str(group.name)\n",
    "        evlat = full_event_df.loc[full_event_df[\"event_ID\"] == event, \"lat\"].values[0]\n",
    "        evlon = full_event_df.loc[full_event_df[\"event_ID\"] == event, \"lon\"].values[0]\n",
    "        event_coords = [evlat, evlon]\n",
    "    \n",
    "        for station in group[\"stat_ID\"]:\n",
    "            \n",
    "            stalat = station_df.loc[station_df[\"stat_ID\"] == station, \"lat\"].values[0]\n",
    "            stalon = station_df.loc[station_df[\"stat_ID\"] == station, \"lon\"].values[0]\n",
    "            stat_coords = [stalat, stalon]\n",
    "        \n",
    "            azimuth = calculate_azimuth(stat_coords, event_coords)\n",
    "            azimuths.append(azimuth)\n",
    "    \n",
    "        max_gap = calculate_azimuthal_gap(azimuths)\n",
    "        return max_gap\n",
    "    \n",
    "    if calculate_max_gap_P == True:\n",
    "        \n",
    "        df = all_picks_df[all_picks_df[\"Phase\"] == \"P\"]\n",
    "        grouped_df = df.groupby('event_ID')\n",
    "        \n",
    "        # apply the function to each group to calculate the max_gap\n",
    "        max_gap_series = grouped_df.apply(calculate_max_gap)\n",
    "\n",
    "        # add the max_gap values to the pick df\n",
    "        df[\"max_gap\"] = df[\"event_ID\"].map(max_gap_series)\n",
    "\n",
    "        # filter out groups with max_gap higher than 180\n",
    "        filtered_df = df[df[\"max_gap\"] <= recalc_max_gap]\n",
    "        \n",
    "        # retrieve S-data\n",
    "        all_picks_df = all_picks_df[all_picks_df[\"event_ID\"].isin(filtered_df[\"event_ID\"])]\n",
    "        upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "        \n",
    "        print(\"Total no of events left after removing high P-arrival azimuthal gap events: \", len(upd_grouped_df))\n",
    "        print(\"Total no of picks left after removing high P-arrival azimuthal gap events: \", len(all_picks_df))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # apply the function to each group to calculate the max_gap\n",
    "        max_gap_series = grouped_df.apply(calculate_max_gap)\n",
    "\n",
    "        # add the max_gap values to the pick df\n",
    "        df[\"max_gap\"] = df[\"event_ID\"].map(max_gap_series)\n",
    "\n",
    "        # filter out groups with max_gap higher than 180\n",
    "        filtered_df = df[df[\"max_gap\"] <= recalc_max_gap]\n",
    "\n",
    "        all_picks_df = filtered_df\n",
    "        upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "\n",
    "        print(\"Total no of events left after removing high azimuthal gap events: \", len(upd_grouped_df))\n",
    "        print(\"Total no of picks left after removing high azimuthal gap events: \", len(all_picks_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of P-picks:  14165\n",
      "No of S-picks:  13353\n"
     ]
    }
   ],
   "source": [
    "# CHECK HOW MANY EVENTS LEFT HAVE RMS UNCERTAINTY BELOW THRESHOLD AND FILTER\n",
    "\n",
    "if recalc_RMS_filtering == True:\n",
    "    \n",
    "    # function to calulate weighted RMS error\n",
    "    def weighted_rms_error(residuals, weights):\n",
    "        residuals = np.array(residuals, dtype=np.float64)\n",
    "        weights = np.array(weights, dtype=np.float64)\n",
    "        squared_errors = np.square(residuals)\n",
    "        weighted_squared_errors = np.multiply(weights, squared_errors)\n",
    "        weighted_rms = np.sqrt(np.sum(weighted_squared_errors) / np.sum(weights))\n",
    "        return weighted_rms\n",
    "    \n",
    "    # function to calculate the RMS for each event from the remaining picks\n",
    "    def calc_rms(group):\n",
    "        \n",
    "        event = str(group.name)\n",
    "        \n",
    "        # get event pick data from dictionary\n",
    "        key = (\"StaData_\" + event)        \n",
    "        sta_data = sta_data_dict[key]\n",
    "        \n",
    "        # apply a mask to this dataframe, excluding picks which are not in the updated all_picks_df\n",
    "        sta_phase_comb = set(zip(group['stat_ID'], group['Phase']))\n",
    "        mask = sta_data.apply(lambda row: (row['stat_ID'], row['Phase']) in sta_phase_comb, axis=1)\n",
    "        sta_data_filtered = sta_data[mask]\n",
    "        \n",
    "        diff = len(sta_data) - len(sta_data_filtered)\n",
    "        if diff != 0:\n",
    "            print(\"No of picks removed is \", diff, \" for event \", event)\n",
    "        \n",
    "        # initialise lists to store residuals and weights\n",
    "        residuals = []\n",
    "        weights = []\n",
    "        \n",
    "        # get pick residuals and weights\n",
    "        for index, row in sta_data_filtered.iterrows():\n",
    "            \n",
    "            # get pick residuals and weights\n",
    "            pick_residual = row.loc[\"Res\"]\n",
    "            pick_weight = row.loc[\"Weight\"]\n",
    "            \n",
    "            residuals.append(pick_residual)\n",
    "            weights.append(pick_weight)\n",
    "            \n",
    "        # calculate the new RMS error for the event\n",
    "        rms = weighted_rms_error(residuals, weights)\n",
    "        return rms\n",
    "    \n",
    "    # group dataframe by event\n",
    "    df = all_picks_df\n",
    "    grouped_df = df.groupby('event_ID')\n",
    "    \n",
    "    print(\"Total no of events before removing events with high RMS error: \", len(grouped_df))  \n",
    "    print(\"Total no of picks before removing events with high RMS error: \", len(df))\n",
    "    \n",
    "    # apply the function to each group to calculate the new rms error\n",
    "    rms_series = grouped_df.apply(calc_rms)\n",
    "\n",
    "    # add the rms values to the pick df\n",
    "    df[\"new_rms\"] = df[\"event_ID\"].map(rms_series)\n",
    "    \n",
    "    # filter out groups with rms higher than specified value\n",
    "    filtered_df = df[df[\"new_rms\"] < recalc_RMS_max]\n",
    "\n",
    "    # drop the rms column\n",
    "    #filtered_df = filtered_df.drop(columns=[\"rms\"])\n",
    "\n",
    "    all_picks_df = filtered_df\n",
    "    upd_grouped_df = all_picks_df.groupby('event_ID')\n",
    "\n",
    "    print(\"Total no of events left after removing high RMS events: \", len(upd_grouped_df))\n",
    "    print(\"Total no of picks left after removing high RMS events: \", len(all_picks_df))\n",
    "\n",
    "no_P_picks = len(all_picks_df[all_picks_df[\"Phase\"] == \"P\"])\n",
    "no_S_picks = len(all_picks_df[all_picks_df[\"Phase\"] == \"S\"])\n",
    "\n",
    "print(\"No of P-picks: \", no_P_picks)\n",
    "print(\"No of S-picks: \", no_S_picks)\n",
    "\n",
    "#print(all_picks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of events:  1302\n"
     ]
    }
   ],
   "source": [
    "# SAVE ALL FINAL PICKS AND EVENTS\n",
    "\n",
    "# save picks to file\n",
    "all_picks_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"picks_final.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "\n",
    "# save events to file\n",
    "final_event_list = []\n",
    "for event in event_df[\"event_ID\"]:\n",
    "    final_event = event in all_picks_df['event_ID'].values\n",
    "    if final_event == True:\n",
    "        final_event_list.append(event)\n",
    "        gap = all_picks_df.loc[all_picks_df[\"event_ID\"] == event, \"max_gap\"].values[0]\n",
    "        event_df.loc[event_df[\"event_ID\"] == event, \"Gap\"] = gap\n",
    "        \n",
    "mask = (event_df[\"event_ID\"].isin(final_event_list))\n",
    "event_df = event_df[mask]\n",
    "event_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "event_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"events_final.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "\n",
    "print(\"Final number of events: \", len(final_event_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE ANY PREEXISTING PICK FILES FROM THE FMTOMO INPUTS FOLDER\n",
    "\n",
    "files = os.listdir(path_to_fmtomo_files)\n",
    "if files:\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path_to_fmtomo_files, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "else:\n",
    "    print(\"Pick file directory is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# WRITE P-PICK FILES PER STATION\n",
    "\n",
    "# group the pick dataframe by event_ID\n",
    "grouped_pick_df = all_picks_df.groupby('event_ID')\n",
    "\n",
    "# initialise dictionary to store the order of events in the list\n",
    "p_station_events = {}\n",
    "\n",
    "# generate pick files\n",
    "\n",
    "P_stations = []\n",
    "for station_id in model_station_id_list:\n",
    "    \n",
    "    station_file_path = os.path.join(path_to_fmtomo_files, '%s.P' %station_id)\n",
    "    pick_df = pd.DataFrame(columns=[\"lat\", \"lon\", \"depth\", \"traveltime\", \"traveltime_err\"])\n",
    "    pick_list = []\n",
    "    \n",
    "    for group_name, group_data in grouped_pick_df:\n",
    "        \n",
    "        # if the station recorded that event, get traveltime and traveltime error for P arrival from the dictionary\n",
    "        mask = (group_data['stat_ID'] == station_id) & (group_data['Phase'] == 'P')\n",
    "        \n",
    "        if mask.any():\n",
    "            \n",
    "            # get traveltime data\n",
    "            traveltime = group_data.loc[mask, 'TT_obs'].values[0]\n",
    "            traveltime_err = group_data.loc[mask, 'TT_err'].values[0]\n",
    "                       \n",
    "            # get event coordinates\n",
    "            lat = group_data.loc[mask, 'lat'].values[0]\n",
    "            lon = group_data.loc[mask, 'lon'].values[0]\n",
    "            depth = group_data.loc[mask, 'depth'].values[0]\n",
    "                \n",
    "            data_row = {\"lat\": lat, \"lon\": lon, \"depth\": depth, \"traveltime\": traveltime, \"traveltime_err\": traveltime_err}\n",
    "            pick_df = pick_df.append(data_row, ignore_index=True)\n",
    "            \n",
    "            # add event_ID to list\n",
    "            pick_list.append(group_name)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # save the pick dataframe into a station pick file (if relevant phase picks exist for that station)\n",
    "    if pick_df.shape[0] != 0:\n",
    "        \n",
    "        pick_df.to_csv(station_file_path, sep=' ', index=False, header=False)\n",
    "    \n",
    "        # calculate number of events and add it to the beginning of the pick file as required by FMTOMO_extras\n",
    "        num_data_rows = len(pick_df)\n",
    "        with open(station_file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        with open(station_file_path, 'w') as file:\n",
    "            file.write(f\"{num_data_rows}\\n\")\n",
    "            file.write(content)\n",
    "            \n",
    "        # add the station pick file name to list\n",
    "        P_stations.append(station_id)\n",
    "        \n",
    "        # save the list of events for this station\n",
    "        key = f\"{station_id}\"\n",
    "        p_station_events[key] = pick_list\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print(len(P_stations))\n",
    "#print(P_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "# WRITE S-PICK FILES PER STATION\n",
    "\n",
    "# group the pick dataframe by event_ID\n",
    "grouped_pick_df = all_picks_df.groupby('event_ID')\n",
    "\n",
    "# initialise dictionary to store the order of events in the list\n",
    "s_station_events = {}\n",
    "\n",
    "# generate pick files\n",
    "S_stations = []\n",
    "\n",
    "for station_id in model_station_id_list:\n",
    "    \n",
    "    station_file_path = os.path.join(path_to_fmtomo_files, '%s.S' %station_id)\n",
    "    pick_df = pd.DataFrame(columns=[\"lat\", \"lon\", \"depth\", \"traveltime\", \"traveltime_err\"])\n",
    "    pick_list = []\n",
    "    \n",
    "    for group_name, group_data in grouped_pick_df:\n",
    "               \n",
    "        # if the station recorded that event, get traveltime and traveltime error for S arrival from the dictionary\n",
    "        mask = (group_data['stat_ID'] == station_id) & (group_data['Phase'] == 'S')\n",
    "        \n",
    "        if mask.any():\n",
    "            # get traveltime data\n",
    "            traveltime = group_data.loc[mask, 'TT_obs'].values[0]\n",
    "            traveltime_err = group_data.loc[mask, 'TT_err'].values[0]\n",
    "                       \n",
    "            # get event coordinates\n",
    "            lat = group_data.loc[mask, 'lat'].values[0]\n",
    "            lon = group_data.loc[mask, 'lon'].values[0]\n",
    "            depth = group_data.loc[mask, 'depth'].values[0]\n",
    "                \n",
    "            data_row = {\"lat\": lat, \"lon\": lon, \"depth\": depth, \"traveltime\": traveltime, \"traveltime_err\": traveltime_err}\n",
    "            pick_df = pick_df.append(data_row, ignore_index=True)\n",
    "            \n",
    "            # add event_ID to list\n",
    "            pick_list.append(group_name)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # save the pick dataframe into a station pick file (if relevant phase picks exist for that station)\n",
    "    if pick_df.shape[0] != 0:\n",
    "        pick_df.to_csv(station_file_path, sep=' ', index=False, header=False)\n",
    "    \n",
    "        # calculate number of events and add it to the beginning of the pick file as required by FMTOMO_extras\n",
    "        num_data_rows = len(pick_df)\n",
    "        with open(station_file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        with open(station_file_path, 'w') as file:\n",
    "            file.write(f\"{num_data_rows}\\n\")\n",
    "            file.write(content)\n",
    "            \n",
    "        # add the station pick file name to list\n",
    "        S_stations.append(station_id)\n",
    "        \n",
    "        # save the list of events for this station\n",
    "        key = f\"{station_id}\"\n",
    "        s_station_events[key] = pick_list\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(len(S_stations))\n",
    "#print(S_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# rearrange the P and S station lists so that each shared station would have the same index\n",
    "#(or the Vp/Vs inversion will not work!)\n",
    "\n",
    "common_stations = list(set(P_stations).intersection(set(S_stations)))\n",
    "print(len(common_stations))\n",
    "\n",
    "P_rearr = common_stations[:]\n",
    "S_rearr = common_stations[:]\n",
    "\n",
    "for P_station in P_stations:\n",
    "    if P_station not in P_rearr:\n",
    "        P_rearr.append(P_station)\n",
    "\n",
    "for S_station in S_stations:\n",
    "    if S_station not in S_rearr:\n",
    "        S_rearr.append(S_station)\n",
    "        \n",
    "S_stations = S_rearr\n",
    "P_stations = P_rearr\n",
    "\n",
    "print(len(S_stations))\n",
    "print(len(P_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE sourceslocal_P.in FILE FOR P-INVERSION\n",
    "\n",
    "P_pick_files = glob.glob(os.path.join(path_to_fmtomo_files, f'*{\".P\"}'))\n",
    "num_P_stations = len(P_pick_files)\n",
    "\n",
    "# reorder the files\n",
    "P_file_path_dict = {os.path.basename(file_path).split('.')[0]: file_path for file_path in P_pick_files}\n",
    "sorted_P_pick_files = [P_file_path_dict[station] for station in P_stations]\n",
    "P_pick_files = sorted_P_pick_files\n",
    "\n",
    "with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_P.in\"), 'w') as file:\n",
    "    file.write(f\"{num_P_stations}\\n\")\n",
    "    \n",
    "for file in P_pick_files:\n",
    "    station_ID = os.path.splitext(os.path.basename(file))[0]\n",
    "    station_row = station_df[station_df.iloc[:, 0] == station_ID]\n",
    "    counter = (P_pick_files.index(file) + 1)\n",
    "    \n",
    "    with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_P.in\"), 'a') as file_to_write:\n",
    "        file_to_write.write(\" \".join(map(str, station_row.iloc[:, [1, 2, 3]].values.flatten())))\n",
    "        file_to_write.write(\" \")\n",
    "        file_to_write.write(str(counter))\n",
    "        file_to_write.write(\"\\n\")\n",
    "        file_to_write.write(\"1\\n\")\n",
    "        file_to_write.write(\"1 1 \")\n",
    "        file_to_write.write(os.path.basename(file))\n",
    "        file_to_write.write(\"\\n\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE sourceslocal_S.in FILE FOR S-INVERSION\n",
    "\n",
    "S_pick_files = glob.glob(os.path.join(path_to_fmtomo_files, f'*{\".S\"}'))\n",
    "num_S_stations = len(S_pick_files)\n",
    "\n",
    "# reorder the files\n",
    "S_file_path_dict = {os.path.basename(file_path).split('.')[0]: file_path for file_path in S_pick_files}\n",
    "sorted_S_pick_files = [S_file_path_dict[station] for station in S_stations]\n",
    "S_pick_files = sorted_S_pick_files\n",
    "\n",
    "with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_S.in\"), 'w') as file:\n",
    "    file.write(f\"{num_S_stations}\\n\")\n",
    "    \n",
    "for file in S_pick_files:\n",
    "    station_ID = os.path.splitext(os.path.basename(file))[0]\n",
    "    station_row = station_df[station_df.iloc[:, 0] == station_ID]\n",
    "    counter = (S_pick_files.index(file) + 1)\n",
    "        \n",
    "    with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_S.in\"), 'a') as file_to_write:\n",
    "        file_to_write.write(\" \".join(map(str, station_row.iloc[:, [1, 2, 3]].values.flatten())))\n",
    "        file_to_write.write(\" \")\n",
    "        file_to_write.write(str(counter))\n",
    "        file_to_write.write(\"\\n\")\n",
    "        file_to_write.write(\"1\\n\")\n",
    "        file_to_write.write(\"1 1 \")\n",
    "        file_to_write.write(os.path.basename(file))\n",
    "        file_to_write.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK EVENT AND STATION ID-S TO FMTOMO ID-S TO GET RESIDUALS LATER (P-DATA)\n",
    "\n",
    "# save station IDs to dataframe\n",
    "\n",
    "p_stat_df = pd.DataFrame(data=None, columns=[\"stat_no\", \"stat_ID\"])\n",
    "p_stat_df[\"stat_no\"] = np.arange(1, (num_P_stations+1))\n",
    "\n",
    "# read sourceslocal_P file to get the station order\n",
    "\n",
    "with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_P.in\"), 'r') as file:\n",
    "    station_lines = file.readlines()\n",
    "\n",
    "for station in p_stat_df['stat_no']:\n",
    "    \n",
    "    station_line = station_lines[station*3]\n",
    "    entries = station_line.split()\n",
    "    station_file = entries[2]\n",
    "    station_id = station_file.split('.')[0]\n",
    "    \n",
    "    p_stat_df.loc[p_stat_df['stat_no'] == station, 'stat_ID'] = station_id\n",
    "\n",
    "#print(p_stat_df)\n",
    "\n",
    "# generate dataframe with all event IDs and FMTOMO IDs matched\n",
    "\n",
    "p_pick_df = pd.DataFrame(data=None, columns=[\"event_no\", \"event_ID\"])\n",
    "p_pick_df[\"event_no\"] = np.arange(1, (no_P_picks+1))\n",
    "\n",
    "all_events = []\n",
    "\n",
    "for stat_id in p_stat_df[\"stat_ID\"]:\n",
    "    \n",
    "    stat_events = p_station_events[stat_id]\n",
    "    #print(len(stat_events))\n",
    "    all_events.extend(stat_events)\n",
    "        \n",
    "p_pick_df[\"event_ID\"] = all_events\n",
    "\n",
    "# save to file\n",
    "p_pick_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"event_id_p.txt\"), sep='\\t', index=False)\n",
    "\n",
    "#print(p_pick_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK EVENT AND STATION ID-S TO FMTOMO ID-S TO GET RESIDUALS LATER (S-DATA)\n",
    "\n",
    "# save station IDs to dataframe\n",
    "\n",
    "s_stat_df = pd.DataFrame(data=None, columns=[\"stat_no\", \"stat_ID\"])\n",
    "s_stat_df[\"stat_no\"] = np.arange(1, (num_S_stations+1))\n",
    "\n",
    "# read sourceslocal_S file to get the station order\n",
    "\n",
    "with open(os.path.join(path_to_fmtomo_files, \"sourceslocal_S.in\"), 'r') as file:\n",
    "    station_lines = file.readlines()\n",
    "\n",
    "for station in s_stat_df['stat_no']:\n",
    "    \n",
    "    station_line = station_lines[station*3]\n",
    "    entries = station_line.split()\n",
    "    station_file = entries[2]\n",
    "    station_id = station_file.split('.')[0]\n",
    "    \n",
    "    s_stat_df.loc[s_stat_df['stat_no'] == station, 'stat_ID'] = station_id\n",
    "\n",
    "# generate dataframe with all event IDs, station IDs and FMTOMO IDs matched\n",
    "\n",
    "s_pick_df = pd.DataFrame(data=None, columns=[\"event_no\", \"event_ID\"])\n",
    "s_pick_df[\"event_no\"] = np.arange(1, (no_S_picks+1))\n",
    "\n",
    "all_events = []\n",
    "\n",
    "for stat_id in s_stat_df[\"stat_ID\"]:\n",
    "    \n",
    "    stat_events = s_station_events[stat_id]\n",
    "    all_events.extend(stat_events)\n",
    "        \n",
    "s_pick_df[\"event_ID\"] = all_events\n",
    "\n",
    "# save to file\n",
    "s_pick_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"event_id_s.txt\"), sep='\\t', index=False)\n",
    "\n",
    "#print(s_pick_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_event_df['lon'] = full_event_df['lon'] - 360\n",
    "event_df['lon'] = event_df['lon'] - 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT EVENT FILES FOR PLOTTING IN GMT\n",
    "\n",
    "full_event_df[\"depth\"] = full_event_df[\"depth\"] * -1\n",
    "#print(full_event_df)\n",
    "# full event catalogue\n",
    "full_event_df.to_csv(os.path.join(path_to_nll_script_outputs, \"all_events.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "full_event_df[['event_ID']].to_csv(os.path.join(path_to_nll_script_outputs, \"all_event_IDs.txt\"), sep='\\t', float_format='%.6f', header=False, index=False)\n",
    "# GMT-friendly file\n",
    "full_event_df[['lon', 'lat']].to_csv(os.path.join(path_to_nll_script_outputs, \"all_events_GMT.xy\"), sep='\\t', float_format='%.6f', header=False, index=False)\n",
    "#print(full_event_df)\n",
    "event_df[\"depth\"] = event_df[\"depth\"] * -1\n",
    "#print(event_df)\n",
    "# events to enter inversion\n",
    "event_df.to_csv(os.path.join(path_to_model_nll_script_outputs, \"filtered_events.txt\"), sep='\\t', float_format='%.6f', index=False)\n",
    "\n",
    "# GMT-friendly file (including pick authors for plotting (0-Tim, 1-Esme, 2-HR))\n",
    "\n",
    "if_esme = event_df['Picker'] == 'Esme'\n",
    "if_tim = event_df['Picker'] == 'Tim'\n",
    "if_hr = event_df['Picker'] == 'Hanna-Riia'\n",
    "\n",
    "event_df.loc[if_esme, 'Picker'] = 0\n",
    "event_df.loc[if_tim, 'Picker'] = 1\n",
    "event_df.loc[if_hr, 'Picker'] = 2\n",
    "\n",
    "event_df[['lon', 'lat', 'depth', 'Picker']].to_csv(os.path.join(path_to_model_nll_script_outputs, \"filtered_events_GMT.xy\"), sep='\\t', float_format='%.6f', header=False, index=False)\n",
    "\n",
    "\n",
    "# filter dataframe to only include the deep cluster\n",
    "deep_df = event_df[(event_df['depth'] <= -7)]\n",
    "deep_df[['lon', 'lat', 'depth']].to_csv(os.path.join(path_to_model_nll_script_outputs, \"deep_events_GMT.xy\"), sep='\\t', float_format='%.6f', header=False, index=False)\n",
    "\n",
    "shallow_df = event_df[(event_df['depth'] >= -7)]\n",
    "shallow_df[['lon', 'lat', 'depth']].to_csv(os.path.join(path_to_model_nll_script_outputs, \"shallow_events_GMT.xy\"), sep='\\t', float_format='%.6f', header=False, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
