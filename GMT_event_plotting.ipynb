{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nonlinloc data conversion script - sources \\\\\n",
    "Hanna-Riia Allas \\\\\n",
    "Oct 11 2023 \\\\\n",
    "\n",
    "This script is for manipulating event data files to create files that could be easily read in by GMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepaths\n",
    "\n",
    "path_to_nnl_outputs = \"/home/hra35/Documents/events/all_picks\" # directory with pick files from nonlinloc\n",
    "path_to_events  = \"/home/hra35/Documents/events\" # directory with all event data\n",
    "path_to_gmt_files = \"/home/hra35/Documents/events/gmt_data\" # directory with files for GMT plots\n",
    "path_to_catalogues = \"/home/hra35/Documents/events/catalogues\" # directory with complete event catalogues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE NONLINLOC EVENT DATAFRAME\n",
    "\n",
    "# make lists of all the nonlinloc output files and pick files in the nonlinloc directory\n",
    "\n",
    "nll_files = glob.glob(os.path.join(path_to_nnl_outputs, f'*{\".hyp\"}'))\n",
    "obs_files = glob.glob(os.path.join(path_to_nnl_outputs, f'*{\".obs\"}'))\n",
    "\n",
    "# iterate through the nonlinloc files to get event id-s from filenames\n",
    "event_id_list = []\n",
    "for file in nll_files:\n",
    "    event_id = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_id_list.append(event_id)\n",
    "    \n",
    "# initialise a dataframe\n",
    "event_df = pd.DataFrame(data=None, index=event_id_list, columns=[\"event_ID\",\n",
    "                                                                 \"lat\",\n",
    "                                                                 \"lon\",\n",
    "                                                                 \"depth\",\n",
    "                                                                 \"RMS\",\n",
    "                                                                 \"no_P_picks\",\n",
    "                                                                 \"no_S_picks\"])\n",
    "\n",
    "for i in range(len(event_id_list)):\n",
    "    event_df.loc[event_id_list[i], [\"event_ID\"]] = [event_id_list[i]]\n",
    "\n",
    "# read the .hyp files and fill in the dataframe with coordinates and rms values\n",
    "\n",
    "for file in nll_files:\n",
    "    with open(file, 'r') as file_to_read:\n",
    "        lines = file_to_read.readlines()\n",
    "        for line in lines:\n",
    "            \n",
    "            if line.startswith(\"GEOGRAPHIC\"):\n",
    "                entries = line.split()\n",
    "                latitude = entries[9]\n",
    "                longitude = entries[11]\n",
    "                depth = entries[13]\n",
    "\n",
    "            if line.startswith(\"QUALITY\"):\n",
    "                entries = line.split()\n",
    "                rms = entries[8]\n",
    "            \n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_df.loc[file_name, [\"lat\", \"lon\", \"depth\", \"RMS\"]] = [latitude, longitude, depth, rms]\n",
    "\n",
    "# read the .obs files to get the number of P and S picks per event\n",
    "\n",
    "for file in obs_files:\n",
    "    df = pd.read_csv(file, delim_whitespace=True, header=None, skiprows=1)\n",
    "    \n",
    "    # check how many P picks exist and extract station IDs  \n",
    "    mask = df.iloc[:, 4] == \"P\"\n",
    "    stations_with_P_picks = df[mask].iloc[:, 0]\n",
    "    no_P = len(stations_with_P_picks)\n",
    "    \n",
    "    # check how many S picks exist and extract station IDs  \n",
    "    mask = df.iloc[:, 4] == \"S\"\n",
    "    stations_with_S_picks = df[mask].iloc[:, 0]\n",
    "    no_S = len(stations_with_S_picks)\n",
    "    \n",
    "    file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    event_df.loc[file_name, [\"no_P_picks\", \"no_S_picks\"]] = [no_P, no_S]\n",
    "\n",
    "# save the complete structured data into a csv file\n",
    "event_df.to_csv(os.path.join(path_to_gmt_data, \"picks_all.csv\"), sep=',', index=False)\n",
    "event_df = pd.read_csv(os.path.join(path_to_gmt_files, \"picks_all.csv\"))\n",
    "event_df['depth'] = event_df['depth']*(-1)\n",
    "event_df.to_csv(os.path.join(path_to_gmt_files, \"picks_all.csv\"), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE QM CATALOGUE DATAFRAMES\n",
    "\n",
    "# list the QM catalogues\n",
    "\n",
    "cat_20_21 = os.path.join(path_to_catalogues, \"events_sorted_2020-21.csv\")\n",
    "cat_21_22 = os.path.join(path_to_catalogues, \"events_sorted_2021-22.csv\")\n",
    "cat_22_23 = os.path.join(path_to_catalogues, \"events_sorted_2022-23.csv\")\n",
    "\n",
    "catalogues = [cat_20_21, cat_21_22, cat_22_23]\n",
    "\n",
    "# read the QM catalogues into dataframes\n",
    "\n",
    "columns_to_read = [\"EventID\", \"DT\", \"X\", \"Y\", \"Z\"]\n",
    "cat_dataframes = {}\n",
    "\n",
    "for catalogue in catalogues:\n",
    "    df = pd.read_csv(catalogue, delimiter=',', skiprows=0, usecols=columns_to_read)\n",
    "    file_name_parts = os.path.splitext(os.path.basename(catalogue))[0].split('_')\n",
    "    df_name = f'df_{file_name_parts[2]}'\n",
    "    df = df.rename(columns={'EventID': 'event_ID', 'DT': 'Date', 'X': 'lon', 'Y': 'lat', 'Z': 'depth'})\n",
    "    df['depth'] = df['depth'] * -1\n",
    "    cat_dataframes[df_name] = df\n",
    "    \n",
    "#print(cat_dataframes['df_2020-21'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE GMT MAP AXES\n",
    "\n",
    "gmt_xaxis = 'lon'\n",
    "gmt_yaxis = 'depth'\n",
    "#gmt_zaxis = 'depth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FILES TO PLOT PICKED DATA WITH GMT\n",
    "\n",
    "# export a .xy file in GMT-compatible format for plotting picked data\n",
    "\n",
    "event_df[[gmt_xaxis, gmt_yaxis]].to_csv(os.path.join(path_to_gmt_files, f\"all_picks_{gmt_xaxis}_{gmt_yaxis}.xy\"), sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FILES TO PLOT CATALOGUE DATA WITH GMT\n",
    "\n",
    "# export .xy files in GMT-compatible format for plotting full catalogue data\n",
    "\n",
    "cat_dataframes['df_2020-21'][[gmt_xaxis,gmt_yaxis]].to_csv(os.path.join(path_to_gmt_data, f\"202021_catalogue_{gmt_xaxis}_{gmt_yaxis}.xy\"), sep='\\t', header=False, index=False)\n",
    "cat_dataframes['df_2021-22'][[gmt_xaxis,gmt_yaxis]].to_csv(os.path.join(path_to_gmt_data, f\"202122_catalogue_{gmt_xaxis}_{gmt_yaxis}.xy\"), sep='\\t', header=False, index=False)\n",
    "cat_dataframes['df_2022-23'][[gmt_xaxis,gmt_yaxis]].to_csv(os.path.join(path_to_gmt_data, f\"202223_catalogue_{gmt_xaxis}_{gmt_yaxis}.xy\"), sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FILES TO PLOT EVENTS FROM A SPECIFIC REGION\n",
    "\n",
    "# define bounds of data region\n",
    "lat_max = 64.0\n",
    "lat_min = 63.75\n",
    "lon_max = -21.8\n",
    "lon_min = -22.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FILE TO PLOT STATIONS\n",
    "\n",
    "path_to_stations = \"/home/hra35/Documents/events/station_coordinates.txt\"\n",
    "\n",
    "# read the file into a dataframe\n",
    "station_df = pd.read_csv(path_to_stations, sep=r'\\s+', names = ['stat_ID', 'rm1', 'lat', 'lon', 'elev_km', 'rm2'])\n",
    "station_df.drop(['rm1', 'rm2', 'elev_km'], axis=1, inplace=True)\n",
    "new_column_order = ['lon', 'lat', 'stat_ID']\n",
    "station_df = station_df[new_column_order]\n",
    "\n",
    "# remove stations that occur twice \n",
    "duplicate_mask = station_df.duplicated(subset='stat_ID', keep='last')\n",
    "station_df = station_df[~duplicate_mask]\n",
    "\n",
    "# export for GMT\n",
    "station_df.to_csv(\"/home/hra35/Documents/gmt_scripts/stations.xy\", index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
